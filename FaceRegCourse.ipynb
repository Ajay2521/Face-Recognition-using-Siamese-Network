{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00e443e",
   "metadata": {},
   "source": [
    "Face Verification \"Is this the claimed person?\" given two images and you have to determine if they are of the same person. \n",
    "- The simplest way to do this is to compare the two images pixel-by-pixel. If the distance between the raw images is below a chosen threshold, it may be the same person! \n",
    "- Of course, this algorithm performs poorly, since the pixel values change dramatically due to variations in lighting, orientation of the person's face, minor changes in head position, and so on.\n",
    "\n",
    "#### rather than using the raw image, you can learn an encoding,  ùëì(ùëñùëöùëî) \n",
    "#### By using an encoding for each image, an element-wise comparison produces a more accurate judgement\n",
    "\n",
    "- Eg A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.\n",
    "\n",
    "Face Recognition \"Who is this person?\" - Eg person details like name. This is a 1:K matching problem. \n",
    "\n",
    "## FaceNet\n",
    "\n",
    "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person.\n",
    "\n",
    "### Tech Used\n",
    "\n",
    "- one-shot learning to solve a face recognition problem\n",
    "- triplet loss function to learn a network's parameters in the context of face recognition\n",
    "- face recognition as a binary classification problem\n",
    "- Map face images into 128-dimensional encodings using a pretrained model\n",
    "\n",
    "- Images will be of shape  (ùëö,ùëõùêª,ùëõùëä,ùëõùê∂) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7222da89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cdfbe572608>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# for manipulating the array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenfromtxt\u001b[0m \u001b[1;31m# Used to load data from a text file, with missing values handled as specified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# Used for data analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;31m# tensorflow for handling tensor values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;31m# Python Imaging Library - Used for opening, manipulating, and saving many different image file formats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# plain stack of layers where each layer has exactly one input tensor and one output tensor, allows to create models layer-by-layer in a step-by-step fashion\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "# Conv2D - used for convolution, layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
    "# ZeroPadding2D - add rows and columns of zeros at the top, bottom, left and right side of an image tensor.\n",
    "# Activation - Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it, purpose of the activation function is to introduce non-linearity into the output of a neuron\n",
    "# Concatenate - data from the input tensors is joined along the axis dimension\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "\n",
    "# Model - groups layers into an object with training and inference features.\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# BatchNormalization - use to normalize the inputs of each layer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# MaxPooling2D - Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size ) for each channel of the input.\n",
    "# AveragePooling2D - Downsamples the input along its spatial dimensions (height and width) by taking the Average value over an input window (of size defined by pool_size ) for each channel of the input.\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# Lambda - Used to constructing Sequential and Functional API models.\n",
    "# Flatten - Used to flatten the data into vector format\n",
    "# Dense - Used to make a Fully connected layer\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "\n",
    "# glorot_uniform - from a uniform distribution within certain limits\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# model_from_json - used to Parses a JSON model configuration string and returns a model instance\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Layer - object that takes as input one or more tensors and that outputs one or more tensors\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Sets the value of the image data format convention\n",
    "K.set_image_data_format('channels_last')\n",
    "import os # for accessing the file from the machine\n",
    "import numpy as np # for manipulating the array\n",
    "from numpy import genfromtxt # Used to load data from a text file, with missing values handled as specified\n",
    "import pandas as pd # Used for data analysis\n",
    "import tensorflow as tf # tensorflow for handling tensor values\n",
    "import PIL # Python Imaging Library - Used for opening, manipulating, and saving many different image file formats\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2df022",
   "metadata": {},
   "source": [
    "## Encoding Face Images into a 128-Dimensional Vector\n",
    "### Using a ConvNet to Compute Encodings\n",
    "\n",
    "- FaceNet model takes a lot of data and a long time to train so we take the weigth which is already trained by others\n",
    "\n",
    "- faceNet network uses 160x160 dimensional RGB images as its input. Specifically, a face image (or batch of  ùëö  face images) as a tensor of shape  (ùëö,ùëõùêª,ùëõùëä,ùëõùê∂)=(ùëö,160,160,3)\n",
    "\n",
    "- The input images are originally of shape 96x96, thus, you need to scale them to 160x160. This is done in the img_to_encoding() function.\n",
    "\n",
    "- The output is a matrix of shape  (ùëö,128)  that encodes each input face image into a 128-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('keras-facenet-h5/model.json', 'r') # Opening the model which is in json format\n",
    "loaded_model_json = json_file.read() # reading the json file\n",
    "json_file.close() # closing the json file\n",
    "model = model_from_json(loaded_model_json) # Loading the model from the readed data from json\n",
    "model.load_weights('keras-facenet-h5/model.h5') # loading the weigth of the model\n",
    "\n",
    "# printing the model input shape and output shape\n",
    "print(model.inputs)\n",
    "print(model.outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde63a4",
   "metadata": {},
   "source": [
    "The triplet loss function formalizes this, and tries to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor, Negative) further apart.\n",
    "\n",
    "For an image  ùë• , its encoding is denoted as  ùëì(ùë•) , where  ùëì  is the function computed by the neural network.\n",
    "\n",
    "Training will use triplets of images  (ùê¥,ùëÉ,ùëÅ) :\n",
    "\n",
    "- A is an \"Anchor\" image--a picture of a person.\n",
    "- P is a \"Positive\" image--a picture of the same person as the Anchor image.\n",
    "- N is a \"Negative\" image--a picture of a different person than the Anchor image\n",
    "\n",
    "You would thus like to minimize the following \"triplet cost\":\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "Here, the notation \"$[z]_+$\" is used to denote $max(z,0)$.\n",
    "\n",
    "Here, the notation \" [ùëß]+ \" is used to denote  ùëöùëéùë•(ùëß,0) .\n",
    "\n",
    "Notes:\n",
    "\n",
    "The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small.\n",
    "The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.\n",
    "ùõº  is called the margin. It's a hyperparameter that you pick manually. You'll use  ùõº=0.2 .\n",
    "\n",
    "Since using a pretrained model, don't need to implement the triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd632f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: triplet_loss\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    \n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    \n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    \n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15180578",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "y_true = (None, None, None) # It is not used\n",
    "y_pred = (tf.keras.backend.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "          tf.keras.backend.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "          tf.keras.backend.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "loss = triplet_loss(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = model # Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddf40d",
   "metadata": {},
   "source": [
    "### Face Verification\n",
    "\n",
    "building a database(represented as a Python dictionary) containing one encoding vector for each person. To generate the encoding, we use img_to_encoding(image_path, model), which runs the forward propagation of the model on the specified image.\n",
    "\n",
    "This database maps each person's name to a 128-dimensional encoding of their face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image_path, model):\n",
    "    \n",
    "    # loading the image \n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n",
    "    # converting the image to pixcel values and around it off\n",
    "    img = np.around(np.array(img) / 255.0, decimals=12)\n",
    "    # expandind the img value for making a fit as the input of the model \n",
    "    x_train = np.expand_dims(img, axis=0)\n",
    "    # predicting the face embedding\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    \n",
    "    return embedding / np.linalg.norm(embedding, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9281e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722429d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "danielle = tf.keras.preprocessing.image.load_img(\"images/danielle.png\", target_size=(160, 160))\n",
    "kian = tf.keras.preprocessing.image.load_img(\"images/kian.jpg\", target_size=(160, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb572594",
   "metadata": {},
   "source": [
    "Verify() function, which checks picture (image_path) is actually the person called \"identity\". You will have to go through the following steps:\n",
    "\n",
    "- Compute the encoding of the image from image_path.\n",
    "- Compute the distance between this encoding and the encoding of the identity image stored in the database.\n",
    "- if the distance is less than 0.7, else do not open it.\n",
    "\n",
    "As presented above, you should use the L2 distance np.linalg.norm.\n",
    "\n",
    "Note: In this implementation, compare the L2 distance, not the square of the L2 distance, to the threshold 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3786e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: verify\n",
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "        door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (‚âà 1 line)\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    # Step 2: Compute distance with identity's image (‚âà 1 line)\n",
    "    dist = np.linalg.norm(encoding-database[identity])\n",
    "    # Step 3: Open the door if dist < 0.7, else don't open (‚âà 3 lines)\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "        \n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c6810",
   "metadata": {},
   "source": [
    "## Face Reg\n",
    "\n",
    "- Compute the target encoding of the image from image_path\n",
    "- Find the encoding from the database that has smallest distance with the target encoding.\n",
    "- Initialize the min_dist variable to a large enough number (100). This helps you keep track of the closest encoding to the input's encoding.\n",
    "- Loop over the database dictionary's names and encodings. To loop use for (name, db_enc) in database.items().\n",
    "- Compute the L2 distance between the target \"encoding\" and the current \"encoding\" from the database. If this distance is less than the min_dist, then set min_dist to dist, and identity to name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: who_is_it\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        database -- database containing image encodings along with the name of the person on the image\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "        identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (‚âà 1 line)\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (‚âà1 line)\n",
    "    min_dist = 100\n",
    "        \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (‚âà 1 line)\n",
    "        dist = np.linalg.norm(encoding-db_enc)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (‚âà 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 with Younes pictures \n",
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cd8d2",
   "metadata": {},
   "source": [
    "Ways to improve your facial recognition model:\n",
    "\n",
    "Although you won't implement these here, here are some ways to further improve the algorithm:\n",
    "\n",
    "Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then, given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.\n",
    "\n",
    "Crop the images to contain just the face, and less of the \"border\" region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c1e37",
   "metadata": {},
   "source": [
    "What you should remember:\n",
    "\n",
    "- Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem.\n",
    "\n",
    "- Triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.-\n",
    "\n",
    "- The same encoding can be used for verification and recognition. Measuring distances between two images' encodings allows you to determine whether they are pictures of the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a02fd",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - References\n",
    "1. Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "\n",
    "2. Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)\n",
    "\n",
    "3. This implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet\n",
    "\n",
    "4. Further inspiration was found here: https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/\n",
    "\n",
    "5. And here: https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aed4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
